{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook is the base for :\n",
    "- Data cleaning\n",
    "- Data Impuation\n",
    "- Feature Engineering\n",
    "\n",
    "## Used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "The train and test inputs are composed of 46 features.\n",
    "\n",
    "The target of this challenge is `RET` and corresponds to the fact that the **return is in the top 50% of highest stock returns**.\n",
    "\n",
    "Since the median is very close to 0, this information should not change much with the idea to predict the sign of the return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('../x_train.csv', index_col='ID')\n",
    "y_train = pd.read_csv('../y_train.csv', index_col='ID')\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "test = pd.read_csv('../x_test.csv', index_col='ID')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "- Cleaning : removing all rows with no observed returns over the past 5 days\n",
    "- Imputation : Simple Impute the median for the remaining NaNs of RET_x and VOLUME_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows with no observed returns over the past 5 days\n",
    "ret_5D_window_cols = [f'RET_{day}' for day in range(1,6)]\n",
    "ret_to_drop = train[(train[ret_5D_window_cols].isna().sum(axis=1)/(train[ret_5D_window_cols].shape[1]) >= 1)][ret_5D_window_cols]\n",
    "\n",
    "train.drop(index=ret_to_drop.index, inplace=True) \n",
    "\n",
    "# Simple Impute the median for the remaining NaNs of RET_x and VOLUME_x\n",
    "ret_cols = [col for col in train.columns if 'RET_' in col]\n",
    "volume_cols = [col for col in train.columns if 'VOLUME_' in col]\n",
    "impute_features = ret_cols + volume_cols\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "train[impute_features] = imputer.fit_transform(train[impute_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "The main drawback in this challenge would be to deal with the noise. To do that, we could create some feature that aggregate features with some statistics. \n",
    "\n",
    "The following cell computes statistics on a given target conditionally to some features. For example, we want to generate a feature that describe the mean of `RET_1` conditionally to the `SECTOR` and the `DATE`.\n",
    "\n",
    "**Ideas of improvement**: change shifts, the conditional features, the statistics, and the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "new_features = []\n",
    "\n",
    "# Aggregating features and their differences w.r.t. the features\n",
    "\n",
    "shifts = [1,2,3,4,5] \n",
    "statistics = {'mean':'mean', 'std':'std','skew': lambda x: skew(x, nan_policy='omit'), 'kurt': lambda x: kurtosis(x, nan_policy='omit'), 'max':'max', 'min':'min'}\n",
    "gb_features_list = [['SECTOR', 'DATE'], ['INDUSTRY_GROUP', 'DATE'], ['INDUSTRY', 'DATE'], ['SUB_INDUSTRY', 'DATE']]\n",
    "target_features = ['RET','VOLUME']\n",
    "for target_feature in target_features:\n",
    "    for [cat, date] in gb_features_list:\n",
    "        tmp_name = cat + '_' + date\n",
    "        for shift in shifts:\n",
    "            for stat_name,stat in statistics.items():\n",
    "                name = f'{target_feature}_{shift}_{tmp_name}_{stat_name}'\n",
    "                feat = f'{target_feature}_{shift}'\n",
    "                new_features.append(name)\n",
    "                for data in [train, test]:\n",
    "                    data[name] = data.groupby([cat, date])[feat].transform(stat)\n",
    "\n",
    "# Cumulative returns over the past x days\n",
    "days = [2,3,4,5]\n",
    "\n",
    "for day in days:\n",
    "    name = f'cumulative_RET_{day}D'\n",
    "    new_features.append(name)\n",
    "    for data in [train, test]: # formula product of 1+ret for each day minus 1\n",
    "        data[name] = data[[f'RET_{i}' for i in range(1,day+1) ]].apply(lambda x: (1+x).prod(), axis=1) - 1\n",
    "\n",
    "weeks = 4\n",
    "\n",
    "\n",
    "# TODO : Add the kurtosis and skewness of the returns and volumes\n",
    "target_features = ['RET', 'VOLUME'] \n",
    "for target_feature in target_features:\n",
    "    for week in range(weeks):\n",
    "        name = f'{target_feature}_STOCK_WEEK_{week+1}'\n",
    "        new_features.append('mean_' + name)\n",
    "        new_features.append('std_' + name)\n",
    "        for data in [train, test]:\n",
    "            data['mean_' + name] = data[[f'{target_feature}_{week*5 + day}' for day in range(1,6)]].mean(axis=1)\n",
    "            data['std_' + name] = data[[f'{target_feature}_{week*5 + day}' for day in range(1,6)]].std(axis=1)\n",
    "       \n",
    "# TODO : Change the way the features is created because the DATE agregation is not relevant for dealing with the differents weeks\n",
    "shifts = [1,2,3,4] \n",
    "statistics = ['sum']\n",
    "gb_features_list = [['SECTOR', 'DATE']]\n",
    "target_features = ['mean_VOLUME_STOCK_WEEK']\n",
    "for target_feature in target_features:\n",
    "    for gb_features in gb_features_list:\n",
    "        tmp_name = '_'.join(gb_features)\n",
    "        for shift in shifts:\n",
    "            for stat in statistics:\n",
    "                name = f'{target_feature}_{shift}_/total_VOLUME_of_SECTOR_DATE'\n",
    "                feat = f'{target_feature}_{shift}'\n",
    "                new_features.append(name)\n",
    "                for data in [train, test]:\n",
    "                    data[name] = data[feat]/data.groupby(gb_features)[feat].transform('sum')\n",
    "\n",
    "shifts = [1,2,3,4] \n",
    "statistics = ['sum'] \n",
    "gb_features_list = [['SECTOR', 'DATE']]\n",
    "target_features = ['mean_RET_STOCK_WEEK']\n",
    "for target_feature in target_features:\n",
    "    for gb_features in gb_features_list:\n",
    "        tmp_name = '_'.join(gb_features)\n",
    "        for shift in shifts:\n",
    "            for stat in statistics:\n",
    "                name = f'{target_feature}_{shift}_/total_RET_of_SECTOR_DATE'\n",
    "                feat = f'{target_feature}_{shift}'\n",
    "                new_features.append(name)\n",
    "                for data in [train, test]:\n",
    "                    data[name] = data[feat]/data.groupby(gb_features)[feat].transform('sum')\n",
    "\n",
    "\n",
    "# TODO : Get back on this notion of momentum\n",
    "\n",
    "weeks = [1,2,3,4]\n",
    "targets = ['RET', 'VOLUME']\n",
    "\n",
    "for target in targets:\n",
    "    for week in weeks: \n",
    "        window_size = 5*week\n",
    "        name = f'{target}_{window_size}_day_momentum'\n",
    "        new_features.append(name)\n",
    "        for data in [train, test]:\n",
    "            _data = data.copy()\n",
    "            rolling_mean_target = _data.groupby(by=['SECTOR', 'DATE'])[[f'{target}_{day}' for day in range(2, window_size+1)]].mean()\n",
    "            target_1_mean = _data.groupby(by=['SECTOR', 'DATE'])[[f'{target}_1']].mean()\n",
    "            target_1_mean_aligned, rolling_mean_target_aligned = target_1_mean.align(rolling_mean_target, axis=0, level='SECTOR')\n",
    "            target_momentum = target_1_mean_aligned.sub(rolling_mean_target_aligned.mean(axis=1), axis=0)\n",
    "            target_momentum.rename(columns={f'{target}_1': name},inplace=True)\n",
    "            placeholder = _data.join(target_momentum, on=['SECTOR', 'DATE'], how='left')\n",
    "            data[name] = placeholder[name]\n",
    "\n",
    "\n",
    "\n",
    "weeks = [1,2,3,4]\n",
    "targets = ['RET', 'VOLUME']\n",
    "\n",
    "for week in weeks: \n",
    "    window_size = 5*week\n",
    "    for target in targets: \n",
    "        name = f'{window_size}_day_mean_{target}_vola'\n",
    "        new_features.append(name)\n",
    "        for data in [train, test]:\n",
    "            rolling_std_target = data.groupby(by=['SECTOR', 'DATE'])[[f'{target}_{day}' for day in range(1,window_size+1)]].mean().std(axis=1).to_frame(name)\n",
    "            placeholder = data.join(rolling_std_target, on=['SECTOR', 'DATE'], how='left')\n",
    "            data[name] = placeholder[name]\n",
    "\n",
    "# do the same with the stock granularity or other categories of aggregation\n",
    "\n",
    "targets = [\"RET\"]\n",
    "window_size = [5, 10, 15, 20]\n",
    "\n",
    "for window in window_size:\n",
    "    name = f\"RSI_SECTOR_DATE_{window}\"\n",
    "    new_features.append(name)\n",
    "    for target in targets:\n",
    "        for data in [train, test]:\n",
    "            _data = data.copy()\n",
    "            avg_gain_sector_day = _data.groupby(by=['SECTOR', 'DATE'])[[f'{\"RET\"}_{day}' for day in range(1, window+1)]].mean().agg(lambda x: x[x>0].mean(), axis=1)\n",
    "            avg_loss_sector_day = _data.groupby(by=['SECTOR', 'DATE'])[[f'{\"RET\"}_{day}' for day in range(1, window+1)]].mean().agg(lambda x: x[x<0].mean(), axis=1).abs()\n",
    "            rs_sector_day = avg_gain_sector_day/avg_loss_sector_day\n",
    "            rsi_sector_date = 100 - 100/(1+rs_sector_day)\n",
    "            placeholder = _data.join(rsi_sector_date.to_frame(name), on=['SECTOR', 'DATE'], how='left')\n",
    "            data[name] = placeholder[name]\n",
    "\n",
    "# do the same with the stock granularity or other categories of aggregation\n",
    "window_size = [5, 10, 15, 20]\n",
    "for window in window_size:\n",
    "    name = f'ADL_{window}'\n",
    "    new_features.append(name)\n",
    "    for data in [train, test]:\n",
    "        _data = data.copy()\n",
    "        sum_adl = ((_data.groupby(by=[\"SECTOR\", \"DATE\"])[[f'RET_{day}' for day in range(1, window+1)]]).apply(lambda x: (x>0).sum()) - (_data.groupby(by=[\"SECTOR\", \"DATE\"])[[f'RET_{day}' for day in range(1, window+1)]]).apply(lambda x: (x<0).sum())).sum(axis=1)\n",
    "        placeholder = _data.join(sum_adl.to_frame(name), on=['SECTOR', 'DATE'], how='left')\n",
    "        data[name] = placeholder[name]\n",
    "\n",
    "# ADDITIONAL FEATURES\n",
    "\n",
    "def compute_moving_avg(df, cols):\n",
    "    return df[cols].mean(axis=1)\n",
    "   \n",
    "def compute_volatility(df, cols):\n",
    "    return df[cols].std(axis=1)\n",
    "\n",
    "def compute_ema(df, cols, span=5):\n",
    "    return df[cols].ewm(span=span, axis=1).mean().iloc[:, -1]\n",
    "\n",
    "def compute_momentum(df, col_start, col_end):\n",
    "    return df[col_end] - df[col_start]\n",
    "\n",
    "def compute_relative_volume(df, cols, last_col):\n",
    "    return df[last_col] / df[cols].median(axis=1)\n",
    "\n",
    "def compute_rsi(df, cols):\n",
    "    gains = df[cols].clip(lower=0).mean(axis=1)\n",
    "    losses = df[cols].clip(upper=0).abs().mean(axis=1)\n",
    "    return 100 - (100 / (100 + (gains / losses)))\n",
    "\n",
    "def compute_vw_ret(df, ret_cols, vol_cols):\n",
    "    return (df[ret_cols] * df[vol_cols]).sum(axis=1) / df[vol_cols].sum(axis=1)\n",
    "\n",
    "def generate_indicators(df, apply_to=['RET', 'VOLUME'], num_days = 20):\n",
    "    ind_columns = {}\n",
    "\n",
    "    feature_functions = {\n",
    "        'MA': compute_moving_avg,\n",
    "        'VOLATILITY': compute_volatility,\n",
    "        'EMA': compute_ema,\n",
    "        'MOMENTUM': compute_momentum,\n",
    "        'REL_VOL': compute_relative_volume,\n",
    "        'RSI': compute_rsi\n",
    "    }\n",
    "    \n",
    "    for feature in apply_to:\n",
    "        cols = [f'{feature}_{i}' for i in range(1, num_days+1)]\n",
    "        ind_columns[f'MA_{feature}'] = feature_functions['MA'](df, cols)\n",
    "        ind_columns[f'VOLATILITY_{feature}'] = feature_functions['VOLATILITY'](df, cols)\n",
    "        ind_columns[f'EMA_{feature}'] = feature_functions['EMA'](df, cols)\n",
    "        ind_columns[f'MOMENTUM_{feature}'] = feature_functions['MOMENTUM'](df, f'{feature}_1', f'{feature}_{num_days}')\n",
    "        \n",
    "        if feature == 'VOLUME':\n",
    "            ind_columns['REL_VOL'] = feature_functions['REL_VOL'](df, cols, f'{feature}_{num_days}')\n",
    "        \n",
    "        if feature == 'RET':\n",
    "            ind_columns['RSI_RET'] = feature_functions['RSI'](df, cols)\n",
    "\n",
    "    if 'RET' in apply_to and 'VOLUME' in apply_to:\n",
    "        ind_columns['VW_RET'] = compute_vw_ret(df, [f'RET_{i}' for i in range(1, num_days+1)], [f'VOLUME_{i}' for i in range(1, num_days+1)])\n",
    "\n",
    "    for shift in [5, 10, 20]:\n",
    "        ret, vol = df[[f'RET_{i}' for i in range(1, shift+1)]], df[[f'VOLUME_{i}' for i in range(1, shift+1)]]\n",
    "        # returns statistics\n",
    "        ind_columns[f'Mean_RET_{shift}'] = np.mean(ret, axis=1)\n",
    "        ind_columns[f'Std_RET_{shift}'] = np.std(ret, axis=1)\n",
    "        ind_columns[f'Range_RET_{shift}'] = (lambda x: np.max(x, axis=1) - np.min(x, axis=1))(ret)\n",
    "        ind_columns[f'Momentum_RET_{shift}'] = ret.iloc[:, -1] - ret.iloc[:, 0]\n",
    "        ind_columns[f'Cumulative_RET_{shift}'] = np.prod(1 + ret, axis=1) - 1\n",
    "        ind_columns[f'Skew_RET_{shift}'] = skew(ret, nan_policy='omit', axis=1)\n",
    "        ind_columns[f'Kurtosis_RET_{shift}'] = kurtosis(ret, nan_policy='omit', axis=1)\n",
    "\n",
    "        # volumes statistics\n",
    "        ind_columns[f'Mean_VOL_{shift}'] = np.mean(vol, axis=1)\n",
    "        ind_columns[f'Std_VOL_{shift}'] = np.std(vol, axis=1)\n",
    "        ind_columns[f'Skew_VOL_{shift}'] = skew(vol, nan_policy='omit', axis=1)\n",
    "        ind_columns[f'Kurtosis_VOL_{shift}'] = kurtosis(vol, nan_policy='omit', axis=1)\n",
    "        # correlation between returns and volumes\n",
    "        ind_columns[f'Corr_RET_VOL_{shift}'] = [np.corrcoef(ret.iloc[i], vol.iloc[i])[0, 1] for i in range(len(ret))]\n",
    "\n",
    "    # compute volume surge (VOL_1 - Mean_VOL_20) / Std_VOL_20\n",
    "    ind_columns['VOL_SURGE_5'] = (df['VOLUME_1'] - ind_columns['Mean_VOL_5']) / ind_columns['Std_VOL_5']\n",
    "    ind_columns['VOL_SURGE_10'] = (df['VOLUME_1'] - ind_columns['Mean_VOL_10']) / ind_columns['Std_VOL_10']\n",
    "    ind_columns['VOL_SURGE_20'] = (df['VOLUME_1'] - ind_columns['Mean_VOL_20']) / ind_columns['Std_VOL_20']\n",
    "     \n",
    "    return ind_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputing the extended dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, pd.DataFrame(generate_indicators(train))], axis=1)\n",
    "test = pd.concat([test, pd.DataFrame(generate_indicators(test))], axis=1)\n",
    "\n",
    "# Save the new datasets\n",
    "train.to_csv('../train_extended.csv')\n",
    "test.to_csv('../test_extended.csv')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
